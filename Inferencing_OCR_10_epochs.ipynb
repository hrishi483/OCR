{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# np.random.seed(42)\n",
        "# tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "ZyYRRhmTh0Xq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model(\"/content/drive/MyDrive/Handwritting OCR 10 epochs\")\n"
      ],
      "metadata": {
        "id": "RqCWgP2bizx-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/sayakpaul/Handwriting-Recognizer-in-Keras/releases/download/v1.0.0/IAM_Words.zip\n",
        "!unzip -qq IAM_Words.zip\n",
        "!\n",
        "!mkdir data\n",
        "!mkdir data/words\n",
        "!tar -xf IAM_Words/words.tgz -C data/words\n",
        "!mv IAM_Words/words.txt data\n",
        "!head -20 data/words.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pnbmnphjSSv",
        "outputId": "76494533-84b8-43cd-a029-3a3dc8c685ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace IAM_Words/words.tgz? [y]es, [n]o, [A]ll, [N]one, [r]ename: mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/words’: File exists\n",
            "^C\n",
            "mv: cannot stat 'IAM_Words/words.txt': No such file or directory\n",
            "#--- words.txt ---------------------------------------------------------------#\n",
            "#\n",
            "# iam database word information\n",
            "#\n",
            "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
            "#\n",
            "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
            "#     ok              -> result of word segmentation\n",
            "#                            ok: word was correctly\n",
            "#                            er: segmentation of word can be bad\n",
            "#\n",
            "#     154             -> graylevel to binarize the line containing this word\n",
            "#     1               -> number of components for this word\n",
            "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
            "#     AT              -> the grammatical tag for this word, see the\n",
            "#                        file tagset.txt for an explanation\n",
            "#     A               -> the transcription for this word\n",
            "#\n",
            "a01-000u-00-00 ok 154 408 768 27 51 AT A\n",
            "a01-000u-00-01 ok 154 507 766 213 48 NN MOVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQsMleDsjSP-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
        "for line in words:\n",
        "    if line[0] == \"#\":\n",
        "        continue\n",
        "    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n",
        "        words_list.append(line)\n",
        "\n",
        "len(words_list)\n",
        "\n",
        "np.random.shuffle(words_list)"
      ],
      "metadata": {
        "id": "ECNjhfEDjLml"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_idx]\n",
        "test_samples = words_list[split_idx:]\n",
        "\n",
        "val_split_idx = int(0.5 * len(test_samples))\n",
        "validation_samples = test_samples[:val_split_idx]\n",
        "test_samples = test_samples[val_split_idx:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(\n",
        "    test_samples\n",
        ")\n",
        "\n",
        "print(f\"Total training samples: {len(train_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_samples)}\")\n",
        "print(f\"Total test samples: {len(test_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17XUieEqjLjv",
        "outputId": "574d43a3-4b76-425f-97ae-7d7fbb9fc32c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 86810\n",
            "Total validation samples: 4823\n",
            "Total test samples: 4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = os.path.join(base_path, \"words\")\n",
        "\n",
        "\n",
        "def get_image_paths_and_labels(samples):\n",
        "    paths = []\n",
        "    corrected_samples = []\n",
        "    for (i, file_line) in enumerate(samples):\n",
        "        line_split = file_line.strip()\n",
        "        line_split = line_split.split(\" \")\n",
        "\n",
        "        # Each line split will have this format for the corresponding image:\n",
        "        # part1/part1-part2/part1-part2-part3.png\n",
        "        image_name = line_split[0]\n",
        "        partI = image_name.split(\"-\")[0]\n",
        "        partII = image_name.split(\"-\")[1]\n",
        "        img_path = os.path.join(\n",
        "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
        "        )\n",
        "        if os.path.getsize(img_path):\n",
        "            paths.append(img_path)\n",
        "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
        "\n",
        "    return paths, corrected_samples\n",
        "\n",
        "\n",
        "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
        "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
        "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
      ],
      "metadata": {
        "id": "RuI5VT4cjLfV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find maximum length and the size of the vocabulary in the training data.\n",
        "train_labels_cleaned = []\n",
        "characters = set()\n",
        "max_len = 0\n",
        "\n",
        "for label in train_labels:\n",
        "    label = label.split(\" \")[-1].strip()\n",
        "    for char in label:\n",
        "        characters.add(char)\n",
        "\n",
        "    max_len = max(max_len, len(label))\n",
        "    train_labels_cleaned.append(label)\n",
        "\n",
        "characters = sorted(list(characters))\n",
        "\n",
        "print(\"Maximum length: \", max_len)\n",
        "print(\"Vocab size: \", len(characters))\n",
        "\n",
        "# Check some label samples.\n",
        "train_labels_cleaned[:10]\n",
        "\n",
        "\n",
        "def clean_labels(labels):\n",
        "    cleaned_labels = []\n",
        "    for label in labels:\n",
        "        label = label.split(\" \")[-1].strip()\n",
        "        cleaned_labels.append(label)\n",
        "    return cleaned_labels\n",
        "\n",
        "\n",
        "validation_labels_cleaned = clean_labels(validation_labels)\n",
        "test_labels_cleaned = clean_labels(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T90vCZq7jLWA",
        "outputId": "e8043f87-038b-4a2d-bb96-7b51f259df1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length:  21\n",
            "Vocab size:  78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Mapping characters to integers.\n",
        "char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n",
        "\n",
        "# Mapping integers back to original characters.\n",
        "num_to_char = StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "s8xoYwZPjEnp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "padding_token = 99\n",
        "image_width = 128\n",
        "image_height = 32\n",
        "max_len = 21\n",
        "\n",
        "def distortion_free_resize(image, img_size):\n",
        "    w, h = img_size\n",
        "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
        "\n",
        "    # Check tha amount of padding needed to be done.\n",
        "    pad_height = h - tf.shape(image)[0]\n",
        "    pad_width = w - tf.shape(image)[1]\n",
        "\n",
        "    # Only necessary if you want to do same amount of padding on both sides.\n",
        "    if pad_height % 2 != 0:\n",
        "        height = pad_height // 2\n",
        "        pad_height_top = height + 1\n",
        "        pad_height_bottom = height\n",
        "    else:\n",
        "        pad_height_top = pad_height_bottom = pad_height // 2\n",
        "\n",
        "    if pad_width % 2 != 0:\n",
        "        width = pad_width // 2\n",
        "        pad_width_left = width + 1\n",
        "        pad_width_right = width\n",
        "    else:\n",
        "        pad_width_left = pad_width_right = pad_width // 2\n",
        "\n",
        "    image = tf.pad(\n",
        "        image,\n",
        "        paddings=[\n",
        "            [pad_height_top, pad_height_bottom],\n",
        "            [pad_width_left, pad_width_right],\n",
        "            [0, 0],\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    image = tf.transpose(image, perm=[1, 0, 2])\n",
        "    image = tf.image.flip_left_right(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def preprocess_image(image_path, img_size=(image_width, image_height)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, 1)\n",
        "    image = distortion_free_resize(image, img_size)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "# A utility function to decode the output of the network.\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search.\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
        "        :, :max_len\n",
        "    ]\n",
        "    # Iterate over the results and get back the text.\n",
        "    output_text = []\n",
        "    for res in results:\n",
        "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
        "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(res)\n",
        "    return output_text\n",
        "\n"
      ],
      "metadata": {
        "id": "b5b_nbhwiJKd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45j9JFmRhiVR",
        "outputId": "1536bed1-156f-477c-f00f-7049a8420ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Predicted Text: spaee\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming build_model and preprocess_image are defined in your code\n",
        "prediction_model = keras.models.Model(\n",
        "    loaded_model.get_layer(name=\"image\").input, loaded_model.get_layer(name=\"dense2\").output\n",
        ")\n",
        "\n",
        "# Load and preprocess the new image\n",
        "new_image_path = \"/content/Screenshot 2023-12-03 142945.png\"\n",
        "new_image = preprocess_image(new_image_path)\n",
        "\n",
        "\n",
        "# Expand dimensions to match the model's expected input shape\n",
        "new_image = np.expand_dims(new_image, axis=0)\n",
        "\n",
        "# Get predictions\n",
        "preds = prediction_model.predict(new_image)\n",
        "\n",
        "# Decode the predictions\n",
        "decoded_text = decode_batch_predictions(preds)[0]\n",
        "\n",
        "# Display the results\n",
        "print(\"Predicted Text:\", decoded_text.strip(\"[UNK]\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "laygeF-XiOZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}